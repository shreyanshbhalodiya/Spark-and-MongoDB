{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algae Services\n"
     ]
    }
   ],
   "source": [
    "print ('Algae Services')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1595962738.0714228\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "print (time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan  4 22:30:29 2021\n"
     ]
    }
   ],
   "source": [
    "#Time class and usage\n",
    "import time;  # This is required to include time module.\n",
    "print (time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n"
     ]
    }
   ],
   "source": [
    "# Import SparkContext class\n",
    "from pyspark import SparkContext\n",
    "# Create or use existing sparkcontext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print (sc)\n",
    "#local* : no of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.stop of <SparkContext master=local[*] appName=PySparkShell>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop spark context \n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext.getOrCreate()\n",
    "print (sc)\n",
    "sc.stop # usually didnt stop better restat kernal from GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "# Find spark version\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print (sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SparkConf\n",
    "from pyspark.conf import SparkConf\n",
    "# Create or use existing Spark session\n",
    "ss = SparkSession.builder.config(conf=SparkConf())\n",
    "#SparkSession provides a single point of entry to interact with underlying Spark functionality \n",
    "#A SparkSession can be used create DataFrame, register DataFrame as tables, \n",
    "#execute SQL over tables, cache tables, and read parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession.Builder object at 0x0000022295BCE888>\n"
     ]
    }
   ],
   "source": [
    "#Import SparkConf\n",
    "from pyspark.conf import SparkConf\n",
    "# Create or use existing Spark session\n",
    "ss = SparkSession.builder.config(conf=SparkConf())\n",
    "# View Spark session object\n",
    "print (ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession.Builder object at 0x0000022295BCE888>\n"
     ]
    }
   ],
   "source": [
    "#Ignore, another way to create session\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.config(conf=SparkConf())\n",
    "print (spark)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RDD stands for Resilient Distributed Dataset, these are the elements that run and operate on multiple nodes to do parallel processing on a cluster. RDDs are immutable elements, which means once you create an RDD you cannot change it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "#Creating Plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#Initialize RDD named array\n",
    "array= sc.parallelize(range(10))\n",
    "print (array)\n",
    "#array.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#Initialize RDD named array\n",
    "array= sc.parallelize(range(10))\n",
    "#print (array)\n",
    "array.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create data in Plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "#--Spark version 2.4 and python 3.8 issue\n",
    "#array= sc.parallelize(    range(10))\n",
    "array= sc.parallelize(list(range(10)))\n",
    "# Print data in RDD array\n",
    "#array=str(array)\n",
    "#print (array)\n",
    "array.collect()\n",
    "#type(array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Key value RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[(0, 0), (1, 1), (2, 4), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#kv= sc.parallelize(range(4))\n",
    "kv= sc.parallelize(list(range(4)))\n",
    "print(kv.collect())\n",
    "print(kv.map(lambda x: (x,x*x)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "kv= sc.parallelize(range(4))\n",
    "#kv= sc.parallelize(list(range(4)))\n",
    "#kv.collect()\n",
    "kv.map(lambda x: (x,x*x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "kv= sc.parallelize([(0, 0), (1, 1), (2, 4), (3, 9)])\n",
    "#kv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "kv= sc.parallelize([(0, 0), (1, 1), (2, 4), (3, 9)])\n",
    "kv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "kv= sc.parallelize([(0, 0), (1, 1), (2, 4), (3, 9)])\n",
    "kv.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Movie \\t Shakespeare in Love', 'Directed by\\tJohn Madden', 'Produced by\\t David Parfitt', 'Written by\\t Marc Norman', 'Music by\\tStephen Warbeck']\n"
     ]
    }
   ],
   "source": [
    "# Create Plain RDD of Strings\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array= sc.parallelize(['Movie \t Shakespeare in Love',\n",
    "'Directed by\tJohn Madden','Produced by\t David Parfitt',\n",
    "'Written by\t Marc Norman','Music by\tStephen Warbeck'])\n",
    "array.collect()\n",
    "print(array.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3),\n",
       " ('fleece', 7),\n",
       " ('had', 2),\n",
       " ('lamb', 5),\n",
       " ('little', 4),\n",
       " ('Mary', 1),\n",
       " ('was', 8),\n",
       " ('white', 9),\n",
       " ('whose', 6)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
    "#[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3),\n",
       " ('fleece', 7),\n",
       " ('had', 2),\n",
       " ('lamb', 5),\n",
       " ('little', 4),\n",
       " ('Mary', 1),\n",
       " ('was', 8),\n",
       " ('white', 9),\n",
       " ('whose', 6)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "#sortByKey(Asc=True, partition, keyfunc=lambda )\n",
    "sc.parallelize(tmp2).sortByKey(True, 1, keyfunc=lambda k: k.lower()).collect()\n",
    "#[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD from Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack Overflow  Questions Developer Jobs',\n",
       " 'Tags Users Search� Log In Sign Up',\n",
       " 'Join Stack Overflow to learn, share knowledge, and build your career.',\n",
       " 'Email Sign Up',\n",
       " 'OR SIGN IN WITH']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"C:/Users/saurabhs/Desktop/Python Codes/JupyterCode/dataSet/strings.txt\")\n",
    "data.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/sparkinstall/spark-2.2.0-bin-hadoop2.7/bin/Code/text.txt MapPartitionsRDD[19] at textFile at <unknown>:0\n"
     ]
    }
   ],
   "source": [
    "# Create RDD from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"C:/sparkinstall/spark-2.2.0-bin-hadoop2.7/bin/Code/text.txt\")\n",
    "#data.take(5)\n",
    "#data.collect()\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"C:/sparkinstall/spark-3.0.1-bin-hadoop2.7/bin/text.txt\")\n",
    "\n",
    "#print(data)\n",
    "\n",
    "#data.collect()\n",
    "#data.take(3)\n",
    "data.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Actions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [ 2, 3, 4, 5]\n",
    "sum=0\n",
    "for i in a:\n",
    "    sum =sum +(i*i)\n",
    "    \n",
    "print(sum)\n",
    "----------------------------\n",
    "i=2    --1sec\n",
    "sum = 0+(2*2) => 4 --2sec\n",
    "i=3\n",
    "Sum =4+ (3*3) ==> 13 ---3sec\n",
    "i=4\n",
    "sum = 13+4*4 ==> 29   --4sec\n",
    "i =5\n",
    "sum =29+(5*5) ==> 54 --5sec\n",
    "\n",
    "Mapping :  squaring each value of list\n",
    "----> 2*2    , 3*3 , 4*4 , 5*5\n",
    "\n",
    "[4,9,16,25]\n",
    "--> adding all elements of list   ---reduce\n",
    "4+9 , 16+25     --- 1 sec\n",
    "13 + 41   -- 2sec\n",
    "----> 54\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View data in Plain RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View data in Plain RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array= sc.parallelize(range(10))\n",
    "# Print data in RDD array\n",
    "array.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Plain RDD of Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Movie \\t Shakespeare in Love',\n",
       " 'Directed by\\tJohn Madden',\n",
       " 'Produced by\\t David Parfitt',\n",
       " 'Written by\\t Marc Norman',\n",
       " 'Music by\\tStephen Warbeck']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Plain RDD of Strings\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "string= sc.parallelize(['Movie \t Shakespeare in Love',\n",
    "'Directed by\tJohn Madden', 'Produced by\t David Parfitt',\n",
    "'Written by\t Marc Norman', 'Music by\tStephen Warbeck'])\n",
    "string.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View RDD row count from text file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View RDD count from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"C:/Users/saurabhs/Desktop/Python Codes/JupyterCode/dataSet/strings.txt\")\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack Overflow  Questions Developer Jobs',\n",
       " 'Tags Users Search� Log In Sign Up',\n",
       " 'Join Stack Overflow to learn, share knowledge, and build your career.',\n",
       " 'Email Sign Up',\n",
       " 'OR SIGN IN WITH',\n",
       " ' Google',\n",
       " ' Facebook',\n",
       " 'spark reduce function: understand how it works',\n",
       " '',\n",
       " 'Ask Question',\n",
       " '',\n",
       " 'up vote',\n",
       " '1',\n",
       " 'down vote',\n",
       " 'favorite',\n",
       " '1',\n",
       " 'I am taking this course.',\n",
       " '',\n",
       " 'It says that the reduce operation on RDD is done one machine at a time. That mean if your data is split across 2 computers, then the below function will work on data in the first computer, will find the result for that data and then it will take a single value from second machine, run the function and it will continue that way until it finishes with all values from machine 2. Is this correct?',\n",
       " '',\n",
       " 'I thought that the function will start operating on both machines at the same time and then once it has results from 2 machines, it will again run the function for the last time',\n",
       " '',\n",
       " 'rdd1=rdd.reduce(lambda x,y: x+y)',\n",
       " 'update 1--------------------------------------------',\n",
       " '',\n",
       " 'will below steps give faster answer as compared to reduce function?',\n",
       " '',\n",
       " 'Rdd=[3,5,4,7,4]',\n",
       " 'seqOp = (lambda x, y: x+y)',\n",
       " 'combOp = (lambda x, y: x+y)',\n",
       " 'collData.aggregate(0, seqOp, combOp)',\n",
       " 'Update 2-----------------------------------',\n",
       " '',\n",
       " 'Should both set of codes below execute in same amount time? I checked and it seems that both take the same time.',\n",
       " '',\n",
       " 'import datetime',\n",
       " '',\n",
       " 'data=range(1,1000000000)',\n",
       " 'distData = sc.parallelize(data,4)',\n",
       " 'print(datetime.datetime.now())',\n",
       " 'a=distData.reduce(lambda x,y:x+y)',\n",
       " 'print(a)',\n",
       " 'print(datetime.datetime.now())',\n",
       " '',\n",
       " 'seqOp = (lambda x, y: x+y)',\n",
       " 'combOp = (lambda x, y: x+y)',\n",
       " 'print(datetime.datetime.now())',\n",
       " 'b=distData.aggregate(0, seqOp, combOp)',\n",
       " 'print(b)',\n",
       " 'print(datetime.datetime.now())',\n",
       " 'apache-spark reduce',\n",
       " 'shareimprove this question',\n",
       " \"edited Mar 24 '16 at 19:14\",\n",
       " \"asked Mar 24 '16 at 16:48\",\n",
       " '',\n",
       " 'user2543622',\n",
       " '79082470',\n",
       " 'add a comment',\n",
       " '1 Answer',\n",
       " 'active oldest votes',\n",
       " 'up vote',\n",
       " '3',\n",
       " 'down vote',\n",
       " 'reduce behavior differs a little bit between native (Scala) and guest languages (Python) but simplifying things a little:',\n",
       " '',\n",
       " 'each partition is processed sequentially element by element',\n",
       " 'multiple partitions can be processed at the same time either by a single worker (multiple executor threads) or different workers',\n",
       " 'partial results are fetched to the driver where the final reduction is applied (this is a part which has different implementation in PySpark and Scala)',\n",
       " \"Since it looks like you're using Python lets take a look at the code:\",\n",
       " '',\n",
       " 'reduce creates a simple wrapper for a user provided function:',\n",
       " 'def func(iterator):',\n",
       " '    ...',\n",
       " 'This is wrapper is used to mapPartitions:',\n",
       " '',\n",
       " 'vals = self.mapPartitions(func).collect()',\n",
       " \"It should be obvious this code is embarrassingly parallel and doesn't care how the results are utilized\",\n",
       " 'Collected vals are reduced sequentially on the driver using standard Python reduce:',\n",
       " '',\n",
       " 'reduce(f, vals)',\n",
       " 'where f is a functions passed to RDD.reduce',\n",
       " 'In comparison Scala will merge partial results asynchronously as they come from the workers.',\n",
       " '',\n",
       " 'In case of treeReduce step 3. can performed in a distributed manner as well. See Understanding treeReduce() in Spark',\n",
       " '',\n",
       " 'To summarize reduce, excluding driver side processing, uses exactly the same mechanisms (mapPartitions) as the basic transformations like map or filter, and provide the same level of parallelism (once again excluding driver code). If you have a large number of partitions or f is expensive you can parallelism / distribute final merging using tree* family of methods.',\n",
       " '',\n",
       " 'shareimprove this answer',\n",
       " 'edited May 23 at 12:24',\n",
       " '',\n",
       " 'Community?',\n",
       " '11',\n",
       " \"answered Mar 24 '16 at 17:19\",\n",
       " '',\n",
       " 'zero323',\n",
       " '123k26307406',\n",
       " '  \\t \\t',\n",
       " 'I read your answer. I am having hard time understanding your inputs and also in figuring out whether the statement made in the course is correct or not. It seems that the statement is incorrect based upon \"multiple partitions can be processed at the same time either by a single worker (multiple executor threads) or different workers\". Please provide a direct answer? Please highlight what your are saying using an example - for example RDD is [1,2,3,4,5,6] and [1,2,3] are on one machine and rest of the elements on the other machine..How spark and scala handle these separately? Thanks for ur work � user2543622 Mar 24 \\'16 at 17:48 ',\n",
       " '  \\t \\t',\n",
       " \"I haven't watched the course, so I cannot refer to that, but if they really tell you that it is done machine at the time you've waisted $200. reduce, excluding driver part, is uses the same mechanism are standard Spark transformations, hence exhibits the same parallelism. � zero323 Mar 24 '16 at 17:54\",\n",
       " '  \\t \\t',\n",
       " \"Please highlight what your are saying using an example - for example RDD is [1,2,3,4,5,6] and [1,2,3] are on one machine and rest of the elements on the other machine..How spark and scala handle these differently? Also would it be possible to answer my updated question? � user2543622 Mar 24 '16 at 18:56\",\n",
       " '  \\t \\t',\n",
       " \"a) There should no significant performance difference between aggregate and reduce b) I cannot use an example because in general the order is not deterministic. You can see very crude visualization here but fundamentally operations are not synchronized. 3) About Scala - like I already said - Scala fetches task results asynchronously not by collect. � zero323 Mar 24 '16 at 19:18\",\n",
       " 'add a comment',\n",
       " 'Your Answer',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " 'Sign up or log in',\n",
       " '',\n",
       " ' Sign up using Google',\n",
       " ' Sign up using Facebook',\n",
       " ' Sign up using Email and Password',\n",
       " ' ',\n",
       " 'Post as a guest',\n",
       " '',\n",
       " 'Name',\n",
       " '',\n",
       " 'Email',\n",
       " '',\n",
       " 'required, but never shown',\n",
       " ' Post Your Answer',\n",
       " 'By posting your answer, you agree to the privacy policy and terms of service.',\n",
       " '',\n",
       " \"Not the answer you're looking for? Browse other questions tagged apache-spark reduce or ask your own question.\",\n",
       " '',\n",
       " 'asked',\n",
       " '',\n",
       " '1 year, 8 months ago',\n",
       " '',\n",
       " 'viewed',\n",
       " '',\n",
       " '278 times',\n",
       " '',\n",
       " 'active',\n",
       " '',\n",
       " '1 year, 8 months ago',\n",
       " '',\n",
       " '',\n",
       " 'Looking for a job?',\n",
       " 'Principal Software Engineer',\n",
       " 'IvantiBengaluru, India',\n",
       " 'databasec#',\n",
       " 'Senior Software Engineer (Changing the world with Healthcare Analytics!)',\n",
       " 'Pulse8, Inc.Annapolis, MD',\n",
       " 'REMOTE',\n",
       " 'sql-serverasp.net-mvc',\n",
       " 'Linked',\n",
       " '',\n",
       " '7',\n",
       " 'Understanding treeReduce() in Spark',\n",
       " '3',\n",
       " 'Unexpected results in Spark MapReduce',\n",
       " 'Related',\n",
       " '',\n",
       " '1',\n",
       " 'Spark: groupBy taking lot of time',\n",
       " '0',\n",
       " 'Using Reduce in Apache Spark',\n",
       " '0',\n",
       " 'How to carry-over the calculated value within the RDD ? -Apache spark',\n",
       " '0',\n",
       " 'Spark - joining json RDDs on a specific field (no key-value)',\n",
       " '0',\n",
       " 'Using values of two RDD to generate a new RDD',\n",
       " '0',\n",
       " 'Concurrent operations in spark streaming',\n",
       " '0',\n",
       " 'ReduceByKey function In Spark',\n",
       " '1',\n",
       " 'Spark: understanding the DAG and forcing transformations',\n",
       " '0',\n",
       " 'Confused about the behavior of Reduce function in map reduce',\n",
       " '0',\n",
       " 'pyspark join with function condition',\n",
       " 'Hot Network Questions',\n",
       " '',\n",
       " 'How to replace a green LED with a red one in a simple TTL switch?',\n",
       " 'Python - Sort tuple list with another list',\n",
       " 'Why does man print \"gimme gimme gimme\" at 00:30?',\n",
       " 'Is it rude to wear a sari as a westerner to a wedding in India?',\n",
       " 'How can I extract parts from a ragged nested list?',\n",
       " 'Are there temples where the deity is simply called \"Vishnu\"?',\n",
       " 'File sharing broken after installing Security Update 2017-001',\n",
       " 'Is there something similar to echo -n in heredoc (EOF)?',\n",
       " 'Is it possible to speed up this loop in Python?',\n",
       " 'How to listen and stop giving advice?',\n",
       " 'Can venomous snakes be determined by these pupil, nostril and scale patterns?',\n",
       " 'I live in a desirable locale. How do I handle visiting friends and set healthy boundaries?',\n",
       " \"Are other user's non-empty subdirectories safe from deletion in my directory?\",\n",
       " 'Leaving car with automatic transmission in Neutral instead of Park',\n",
       " 'How do I change the numbering in inner \\\\item?',\n",
       " 'magento2 check if customer is logged in or not in knockout template',\n",
       " \"How to prove that matrix multiplication of two 2x2 matrices can't be done in less than 7 multiplications?\",\n",
       " 'At each step of a limiting infinite process, put 10 balls in an urn and remove one at random. How many balls are left?',\n",
       " 'What is the \"+1 rule\" for Adventurers League?',\n",
       " 'Leaving car for 3 weeks, change oil before or after?',\n",
       " 'Movie about huge spiders from some planet',\n",
       " 'Calling I2C functions inside an ISR',\n",
       " 'Is source code generation an anti-pattern?',\n",
       " 'Can you cut open a plastic junction box and just lay splices inside?',\n",
       " ' question feed',\n",
       " 'STACK OVERFLOW',\n",
       " '',\n",
       " 'Questions',\n",
       " 'Jobs',\n",
       " 'Developer Jobs Directory',\n",
       " 'Salary Calculator',\n",
       " 'Help',\n",
       " 'Mobile',\n",
       " 'STACK OVERFLOW',\n",
       " 'BUSINESS',\n",
       " '',\n",
       " 'Talent',\n",
       " 'Ads',\n",
       " 'Enterprise',\n",
       " 'Insights',\n",
       " 'COMPANY',\n",
       " '',\n",
       " 'About',\n",
       " 'Press',\n",
       " 'Work Here',\n",
       " 'Legal',\n",
       " 'Privacy Policy',\n",
       " 'Contact Us',\n",
       " 'STACK EXCHANGE',\n",
       " 'NETWORK',\n",
       " '',\n",
       " 'Technology',\n",
       " 'Life / Arts',\n",
       " 'Culture / Recreation',\n",
       " 'Science',\n",
       " 'Other',\n",
       " 'Blog Facebook Twitter LinkedIn',\n",
       " 'site design / logo � 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required. rev 2017.11.30.27919',\n",
       " ' Algae Services',\n",
       " 'Algae Services',\n",
       " 'Algae Services',\n",
       " 'Algae Services']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View RDD content from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"C:/Users/saurabhs/Desktop/Python Codes/JupyterCode/dataSet/strings.txt\")\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Random 5 rows inRDD content from text file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack Overflow  Questions Developer Jobs',\n",
       " 'Tags Users Search� Log In Sign Up',\n",
       " 'Join Stack Overflow to learn, share knowledge, and build your career.',\n",
       " 'Email Sign Up',\n",
       " 'OR SIGN IN WITH']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 5 rows inRDD content from text file data\n",
    "sc = SparkContext.getOrCreate()\n",
    "#data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data= sc.textFile(\"C:/Users/saurabhs/Desktop/Python Codes/JupyterCode/dataSet/strings.txt\")\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting values in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting values in RDD\n",
    "a = range(100)  \n",
    "datarange = sc.parallelize(a)\n",
    "datarange.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Random 5 elements from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 5 elements from RDD\n",
    "a = range(100)  \n",
    "datarange = sc.parallelize(a)\n",
    "datarange.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View ordered 5 elements from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 3]\n",
      "[8, 7, 6, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "# View 5 elements from RDD\n",
    "a = range(100)  \n",
    "RDD1 = sc.parallelize(a)\n",
    "RDD2 = sc.parallelize([2,5,3,6,7,2,0,1,5,8])\n",
    "#print (RDD1.top(5))\n",
    "#print (RDD2.top(5))\n",
    "print (RDD2.takeOrdered(5))\n",
    "print (RDD2.top(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating sample data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[4, 9, 3, 1, 0, 0, 6, 7, 4, 6, 8, 9, 3, 2, 0, 2, 3, 2, 5, 8]\n",
      "[5, 9, 3, 4, 6]\n",
      "[1, 5, 6, 0, 9, 4, 7, 2, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(0, 10))\n",
    "print(rdd.collect())\n",
    "print(rdd.takeSample(True, 20, 1))\n",
    "print(rdd.takeSample(False, 5, 2))\n",
    "print(rdd.takeSample(False, 15, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple action on RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack Overflow  Questions Developer Jobs',\n",
       " 'Tags Users Search� Log In Sign Up',\n",
       " 'Join Stack Overflow to learn, share knowledge, and build your career.',\n",
       " 'Email Sign Up',\n",
       " 'OR SIGN IN WITH',\n",
       " ' Google',\n",
       " ' Facebook',\n",
       " 'spark reduce function: understand how it works',\n",
       " '',\n",
       " 'Ask Question',\n",
       " '',\n",
       " 'up vote',\n",
       " '1',\n",
       " 'down vote',\n",
       " 'favorite',\n",
       " '1',\n",
       " 'I am taking this course.',\n",
       " '',\n",
       " 'It says that the reduce operation on RDD is done one machine at a time. That mean if your data is split across 2 computers, then the below function will work on data in the first computer, will find the result for that data and then it will take a single value from second machine, run the function and it will continue that way until it finishes with all values from machine 2. Is this correct?',\n",
       " '',\n",
       " 'I thought that the function will start operating on both machines at the same time and then once it has results from 2 machines, it will again run the function for the last time',\n",
       " '',\n",
       " 'rdd1=rdd.reduce(lambda x,y: x+y)',\n",
       " 'update 1--------------------------------------------',\n",
       " '',\n",
       " 'will below steps give faster answer as compared to reduce function?',\n",
       " '',\n",
       " 'Rdd=[3,5,4,7,4]',\n",
       " 'seqOp = (lambda x, y: x+y)',\n",
       " 'combOp = (lambda x, y: x+y)',\n",
       " 'collData.aggregate(0, seqOp, combOp)',\n",
       " 'Update 2-----------------------------------',\n",
       " '',\n",
       " 'Should both set of codes below execute in same amount time? I checked and it seems that both take the same time.',\n",
       " '',\n",
       " 'import datetime',\n",
       " '',\n",
       " 'data=range(1,1000000000)',\n",
       " 'distData = sc.parallelize(data,4)',\n",
       " 'print(datetime.datetime.now())',\n",
       " 'a=distData.reduce(lambda x,y:x+y)',\n",
       " 'print(a)',\n",
       " 'print(datetime.datetime.now())',\n",
       " '',\n",
       " 'seqOp = (lambda x, y: x+y)',\n",
       " 'combOp = (lambda x, y: x+y)',\n",
       " 'print(datetime.datetime.now())',\n",
       " 'b=distData.aggregate(0, seqOp, combOp)',\n",
       " 'print(b)',\n",
       " 'print(datetime.datetime.now())',\n",
       " 'apache-spark reduce',\n",
       " 'shareimprove this question',\n",
       " \"edited Mar 24 '16 at 19:14\",\n",
       " \"asked Mar 24 '16 at 16:48\",\n",
       " '',\n",
       " 'user2543622',\n",
       " '79082470',\n",
       " 'add a comment',\n",
       " '1 Answer',\n",
       " 'active oldest votes',\n",
       " 'up vote',\n",
       " '3',\n",
       " 'down vote',\n",
       " 'reduce behavior differs a little bit between native (Scala) and guest languages (Python) but simplifying things a little:',\n",
       " '',\n",
       " 'each partition is processed sequentially element by element',\n",
       " 'multiple partitions can be processed at the same time either by a single worker (multiple executor threads) or different workers',\n",
       " 'partial results are fetched to the driver where the final reduction is applied (this is a part which has different implementation in PySpark and Scala)',\n",
       " \"Since it looks like you're using Python lets take a look at the code:\",\n",
       " '',\n",
       " 'reduce creates a simple wrapper for a user provided function:',\n",
       " 'def func(iterator):',\n",
       " '    ...',\n",
       " 'This is wrapper is used to mapPartitions:',\n",
       " '',\n",
       " 'vals = self.mapPartitions(func).collect()',\n",
       " \"It should be obvious this code is embarrassingly parallel and doesn't care how the results are utilized\",\n",
       " 'Collected vals are reduced sequentially on the driver using standard Python reduce:',\n",
       " '',\n",
       " 'reduce(f, vals)',\n",
       " 'where f is a functions passed to RDD.reduce',\n",
       " 'In comparison Scala will merge partial results asynchronously as they come from the workers.',\n",
       " '',\n",
       " 'In case of treeReduce step 3. can performed in a distributed manner as well. See Understanding treeReduce() in Spark',\n",
       " '',\n",
       " 'To summarize reduce, excluding driver side processing, uses exactly the same mechanisms (mapPartitions) as the basic transformations like map or filter, and provide the same level of parallelism (once again excluding driver code). If you have a large number of partitions or f is expensive you can parallelism / distribute final merging using tree* family of methods.',\n",
       " '',\n",
       " 'shareimprove this answer',\n",
       " 'edited May 23 at 12:24',\n",
       " '',\n",
       " 'Community?',\n",
       " '11',\n",
       " \"answered Mar 24 '16 at 17:19\",\n",
       " '',\n",
       " 'zero323',\n",
       " '123k26307406',\n",
       " '  \\t \\t',\n",
       " 'I read your answer. I am having hard time understanding your inputs and also in figuring out whether the statement made in the course is correct or not. It seems that the statement is incorrect based upon \"multiple partitions can be processed at the same time either by a single worker (multiple executor threads) or different workers\". Please provide a direct answer? Please highlight what your are saying using an example - for example RDD is [1,2,3,4,5,6] and [1,2,3] are on one machine and rest of the elements on the other machine..How spark and scala handle these separately? Thanks for ur work � user2543622 Mar 24 \\'16 at 17:48 ',\n",
       " '  \\t \\t',\n",
       " \"I haven't watched the course, so I cannot refer to that, but if they really tell you that it is done machine at the time you've waisted $200. reduce, excluding driver part, is uses the same mechanism are standard Spark transformations, hence exhibits the same parallelism. � zero323 Mar 24 '16 at 17:54\",\n",
       " '  \\t \\t',\n",
       " \"Please highlight what your are saying using an example - for example RDD is [1,2,3,4,5,6] and [1,2,3] are on one machine and rest of the elements on the other machine..How spark and scala handle these differently? Also would it be possible to answer my updated question? � user2543622 Mar 24 '16 at 18:56\",\n",
       " '  \\t \\t',\n",
       " \"a) There should no significant performance difference between aggregate and reduce b) I cannot use an example because in general the order is not deterministic. You can see very crude visualization here but fundamentally operations are not synchronized. 3) About Scala - like I already said - Scala fetches task results asynchronously not by collect. � zero323 Mar 24 '16 at 19:18\",\n",
       " 'add a comment',\n",
       " 'Your Answer',\n",
       " '',\n",
       " '',\n",
       " ' ',\n",
       " 'Sign up or log in',\n",
       " '',\n",
       " ' Sign up using Google',\n",
       " ' Sign up using Facebook',\n",
       " ' Sign up using Email and Password',\n",
       " ' ',\n",
       " 'Post as a guest',\n",
       " '',\n",
       " 'Name',\n",
       " '',\n",
       " 'Email',\n",
       " '',\n",
       " 'required, but never shown',\n",
       " ' Post Your Answer',\n",
       " 'By posting your answer, you agree to the privacy policy and terms of service.',\n",
       " '',\n",
       " \"Not the answer you're looking for? Browse other questions tagged apache-spark reduce or ask your own question.\",\n",
       " '',\n",
       " 'asked',\n",
       " '',\n",
       " '1 year, 8 months ago',\n",
       " '',\n",
       " 'viewed',\n",
       " '',\n",
       " '278 times',\n",
       " '',\n",
       " 'active',\n",
       " '',\n",
       " '1 year, 8 months ago',\n",
       " '',\n",
       " '',\n",
       " 'Looking for a job?',\n",
       " 'Principal Software Engineer',\n",
       " 'IvantiBengaluru, India',\n",
       " 'databasec#',\n",
       " 'Senior Software Engineer (Changing the world with Healthcare Analytics!)',\n",
       " 'Pulse8, Inc.Annapolis, MD',\n",
       " 'REMOTE',\n",
       " 'sql-serverasp.net-mvc',\n",
       " 'Linked',\n",
       " '',\n",
       " '7',\n",
       " 'Understanding treeReduce() in Spark',\n",
       " '3',\n",
       " 'Unexpected results in Spark MapReduce',\n",
       " 'Related',\n",
       " '',\n",
       " '1',\n",
       " 'Spark: groupBy taking lot of time',\n",
       " '0',\n",
       " 'Using Reduce in Apache Spark',\n",
       " '0',\n",
       " 'How to carry-over the calculated value within the RDD ? -Apache spark',\n",
       " '0',\n",
       " 'Spark - joining json RDDs on a specific field (no key-value)',\n",
       " '0',\n",
       " 'Using values of two RDD to generate a new RDD',\n",
       " '0',\n",
       " 'Concurrent operations in spark streaming',\n",
       " '0',\n",
       " 'ReduceByKey function In Spark',\n",
       " '1',\n",
       " 'Spark: understanding the DAG and forcing transformations',\n",
       " '0',\n",
       " 'Confused about the behavior of Reduce function in map reduce',\n",
       " '0',\n",
       " 'pyspark join with function condition',\n",
       " 'Hot Network Questions',\n",
       " '',\n",
       " 'How to replace a green LED with a red one in a simple TTL switch?',\n",
       " 'Python - Sort tuple list with another list',\n",
       " 'Why does man print \"gimme gimme gimme\" at 00:30?',\n",
       " 'Is it rude to wear a sari as a westerner to a wedding in India?',\n",
       " 'How can I extract parts from a ragged nested list?',\n",
       " 'Are there temples where the deity is simply called \"Vishnu\"?',\n",
       " 'File sharing broken after installing Security Update 2017-001',\n",
       " 'Is there something similar to echo -n in heredoc (EOF)?',\n",
       " 'Is it possible to speed up this loop in Python?',\n",
       " 'How to listen and stop giving advice?',\n",
       " 'Can venomous snakes be determined by these pupil, nostril and scale patterns?',\n",
       " 'I live in a desirable locale. How do I handle visiting friends and set healthy boundaries?',\n",
       " \"Are other user's non-empty subdirectories safe from deletion in my directory?\",\n",
       " 'Leaving car with automatic transmission in Neutral instead of Park',\n",
       " 'How do I change the numbering in inner \\\\item?',\n",
       " 'magento2 check if customer is logged in or not in knockout template',\n",
       " \"How to prove that matrix multiplication of two 2x2 matrices can't be done in less than 7 multiplications?\",\n",
       " 'At each step of a limiting infinite process, put 10 balls in an urn and remove one at random. How many balls are left?',\n",
       " 'What is the \"+1 rule\" for Adventurers League?',\n",
       " 'Leaving car for 3 weeks, change oil before or after?',\n",
       " 'Movie about huge spiders from some planet',\n",
       " 'Calling I2C functions inside an ISR',\n",
       " 'Is source code generation an anti-pattern?',\n",
       " 'Can you cut open a plastic junction box and just lay splices inside?',\n",
       " ' question feed',\n",
       " 'STACK OVERFLOW',\n",
       " '',\n",
       " 'Questions',\n",
       " 'Jobs',\n",
       " 'Developer Jobs Directory',\n",
       " 'Salary Calculator',\n",
       " 'Help',\n",
       " 'Mobile',\n",
       " 'STACK OVERFLOW',\n",
       " 'BUSINESS',\n",
       " '',\n",
       " 'Talent',\n",
       " 'Ads',\n",
       " 'Enterprise',\n",
       " 'Insights',\n",
       " 'COMPANY',\n",
       " '',\n",
       " 'About',\n",
       " 'Press',\n",
       " 'Work Here',\n",
       " 'Legal',\n",
       " 'Privacy Policy',\n",
       " 'Contact Us',\n",
       " 'STACK EXCHANGE',\n",
       " 'NETWORK',\n",
       " '',\n",
       " 'Technology',\n",
       " 'Life / Arts',\n",
       " 'Culture / Recreation',\n",
       " 'Science',\n",
       " 'Other',\n",
       " 'Blog Facebook Twitter LinkedIn',\n",
       " 'site design / logo � 2017 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required. rev 2017.11.30.27919',\n",
       " ' ']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "data.count() # count of rows\n",
    "data.take(5) # Return top 5 rows\n",
    "data.collect() # Return coimplete file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition of each element in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Addition of each element in RDD\n",
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "#Combining result from RDD elements\n",
    "print(array.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =[0,1,2,3,4,5,6,7]\n",
    "array = sc.parallelize(a)\n",
    "array.collect()\n",
    "#Combining result from RDD elements\n",
    "#print(array.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "x= sc.parallelize(range(10))\n",
    "print(x.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is below python code faster than Spark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "Wall time: 225 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;\n",
    "x= range(10)\n",
    "print(sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets increase the ammount of data, If you are on custer(multiple nodes) spark will take less otherwise depend on single node cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49995000\n",
      "Wall time: 6.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Addition of each element in RDD\n",
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10000))\n",
    "#Combining result from RDD elements\n",
    "# print(array.collect())\n",
    "print(array.reduce(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499500\n",
      "Wall time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;\n",
    "x= range(1000)\n",
    "print(sum(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find shortest string in String RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find shortest string in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Creating List\n",
    "words = ['This','is','Algae','Code']\n",
    "#Converting list to RDD\n",
    "RDD = sc.parallelize(words)\n",
    "#Combining result from RDD elements\n",
    "RDD.reduce(lambda x,y: x if len(x)< len(y) else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract of each element in RDD could go wrong in case of Multiple nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bad usage\n",
    "# Subtract of each element in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize([1,3,5,2])\n",
    "#Combining result from RDD elements\n",
    "array.reduce(lambda x,y: x-y)\n",
    "#(1-3-5-2) | (1-3-(5-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last word in lexiographic order among longest word in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coool'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last word in lexiographic order among longest word in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Creating List\n",
    "words = ['This','is','Algae','Code','That','was','so','Coool']\n",
    "#Converting list to RDD\n",
    "RDD = sc.parallelize(words)\n",
    "#Combining result from RDD elements\n",
    "def largerThan(x,y):\n",
    "    if len(x)>len(y): return(x)\n",
    "    elif len(x)<len(y): return (y)\n",
    "    else: \n",
    "        if x>y:return (x) \n",
    "        else : return (y)\n",
    "RDD.reduce(largerThan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algae', 'Code', 'Coool', 'That', 'This', 'is', 'so', 'was']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating List\n",
    "words = ['This','is','Algae','Code','That','was','so','Coool']\n",
    "words.sort()\n",
    "print (words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n"
     ]
    }
   ],
   "source": [
    "def largerThan(words,word):\n",
    "    for x in words:\n",
    "        for y in word:\n",
    "            if len(x)>len(y): return(x)\n",
    "            elif len(x)<len(y): return (y)\n",
    "            else: \n",
    "                if x>y:return (x) \n",
    "                else : return (y)\n",
    "                \n",
    "words = ['This','is','Algae','Code','That','was','so','Coool']\n",
    "word = ['is','Algae','Code','That','was','so','Coool']\n",
    "print(largerThan(words,word))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 3, 4, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([0, 2, 3, 4, 6], 10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [2], [3], [4], [6]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "# Glom will covert all element of list into list itself\n",
    "sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
    "#[[0], [2], [3], [4], [6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([1, 2, 3, 4, 5, 6]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "word = ['is','Algae','Code','That','was','so','Coool']\n",
    "sc.parallelize(word).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 8, 6, 8, 7, 8, 9, 7, 5, 8, 3, 9, 6, 3, 2, 1, 0, 4, 7, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#takeSample(withReplacement, num, seed=None)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(range(0, 10))\n",
    "rdd.takeSample(True, 20, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 8, 9, 7, 5, 3, 0, 4, 1, 2]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(range(0, 10))\n",
    "rdd.takeSample(False, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "rdd = sc.parallelize(range(0, 10))\n",
    "rdd.takeSample(False, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 1), ('1', 3), ('d', 4), ('2', 5)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "a=sc.parallelize(tmp)\n",
    "a.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda>>)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
    "#[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda>>)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
    "#[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3),\n",
       " ('fleece', 7),\n",
       " ('had', 2),\n",
       " ('lamb', 5),\n",
       " ('little', 4),\n",
       " ('Mary', 1),\n",
       " ('was', 8),\n",
       " ('white', 9),\n",
       " ('whose', 6)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda>>)\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
    "#[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "#tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "a = sc.parallelize(tmp2).sortByKey(True, 10, keyfunc=lambda k: k.lower())\n",
    "a.getNumPartitions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sc.parallelize([3, 1, 2, 3]).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([3, 1, 2, 3]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#sort by Key\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#sort by Key\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5, -3, -2, -1, 1, 2, 3, 4, 4, 5]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "#sort the output\n",
    "a =[1,-1,2,-2,3,-3,4,4,5,-5]\n",
    "sc.parallelize(a).sortBy(lambda x: x ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by Element\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 3, 2]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Divide data into partition\n",
    "rdd = sc.parallelize([1, 2, 3, 4,5,3,2], 2)\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Square of each element in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Square of each element in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "#applying operation on each element of RDD\n",
    "array.map(lambda x: x*x).collect()\n",
    "#print(array.map(lambda x: x*x).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify time take generating square of each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "#applying operation on each element of RDD\n",
    "array.map(lambda x: x*x).collect()\n",
    "#print(array.map(lambda x: x*x).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validating RDDs are DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
      "Wall time: 6.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from time import time\n",
    "import time;\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "#applying operation on each element of RDD\n",
    "array.map(lambda x: x*x).collect()\n",
    "print(array.map(lambda x: x*x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May 26 21:12:48 2018\n",
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
      "Sat May 26 21:12:51 2018\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Square of each element in RDD\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "#applying operation on each element of RDD\n",
    "print (time.asctime( time.localtime(time.time()) ))\n",
    "print(array.map(lambda x: x*x).collect())\n",
    "print (time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9], [4, 16], [5, 25]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "import time;\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Stack Overflow  Questions Developer Jobs'],\n",
       " ['Tags Users Search� Log In Sign Up']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "#Pick each line as element of RDD\n",
    "b = data.map(lambda line : line.split('/t'))\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "import time;\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack', 'Overflow']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "data= sc.textFile(\"D:/Perseonal/MyTrainings/Spark Training/Spark-Notes/Lab/strings.txt\")\n",
    "# Pick each word as element of RDD\n",
    "a = data.flatMap(lambda line : line.split(\" \"))\n",
    "a.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Stack', 'Overflow', '', 'Questions', 'Developer', 'Jobs'],\n",
       " ['Tags', 'Users', 'Search�', 'Log', 'In', 'Sign', 'Up']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"D:/Perseonal/MyTrainings/Spark Training/Spark-Notes/Lab/strings.txt\")\n",
    "#Pick each line as element of RDD\n",
    "b = data.map(lambda line : line.split(' '))\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack Overflow  Questions Developer Jobs',\n",
       " 'Tags Users Search� Log In Sign Up']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "#Pick each line as element of RDD\n",
    "b = data.flatMap(lambda line : line.split('/t'))\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize (range(10))\n",
    "b= a.filter(lambda x: x>5)\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EFG']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize (['A','BC','D','EFG'])\n",
    "b = a.filter(lambda x: len(x) > 2)\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1--------------------------------------------',\n",
       " '2-----------------------------------',\n",
       " 'self.mapPartitions(func).collect()']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "array = sc.parallelize(range(10))\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "# Pick each word as element of RDD\n",
    "a = data.flatMap(lambda line : line.split(\" \"))\n",
    "b = a.filter(lambda x: len(x) > 30)\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'my']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "a= data.flatMap(lambda lines: lines.split(\" \")).filter(lambda value: value==\"my\")\n",
    "a.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack', 'Overflow']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "b= data.flatMap(lambda lines: lines.split(\" \")).filter(lambda value: value!=\"my\")\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize (range(10))\n",
    "b= a.map(lambda x: x if x%2==0 else '' ).filter(lambda x: x!='')\n",
    "b.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 1, 8, 3]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([5, 1, 8, 3])\n",
    "rdd2 = sc.parallelize(range(5))\n",
    "rdd2.union(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'BC', 'ZSK', 'GH', 'A', 'BC', 'DEF', 'GH']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(['A', 'BC', 'DEF' ,'GH'])\n",
    "rdd2 = sc.parallelize(['A', 'UC', 'ZSK' ,'GH'])\n",
    "rdd2.union(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GH', 'A']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(['A', 'BC', 'DEF' ,'GH'])\n",
    "rdd2 = sc.parallelize(['A', 'UC', 'ZSK' ,'GH'])\n",
    "rdd2.intersection(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([5, 1, 8, 3])\n",
    "rdd2 = sc.parallelize(range(5))\n",
    "rdd2.intersection(rdd1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inner Join\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "x.join(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3)), ('b', (4, None))]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "x.leftOuterJoin(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "x.rightOuterJoin(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stack', 'Overflow']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "def myfun(a):\n",
    "    word = a.split(' ')\n",
    "    return word\n",
    "\n",
    "a = sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "b = a.flatMap(lambda line: myfun(line))\n",
    "b.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY Value RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 10}, {2: 20}, {3: 30}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([ {1: 10},{2:20},{3:30}])\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1, 10}, {2, 20}, {3, 30}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([ {1, 10},{2,20},{3,30}])\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, {'Dept': 'Edu', 'name': 'Saurabh'}),\n",
       " (2, {'Dept': 'Admin', 'name': 'Kunal'})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize(\n",
    "    [(1, {'name': 'Saurabh', 'Dept': 'Edu'}),\n",
    "     ( 2,{'name': 'Kunal',  'Dept':'Admin'})])\n",
    "#data.collect()\n",
    "# Key and key value pair as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Saurabh': 100}, {'Kual': 200}, {'Saurabh': 200}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([ {'Saurabh': 100},{'Kual':200},{'Saurabh':200}])\n",
    "data.collect()\n",
    "# Here key is identifier but it doesnt need to be unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation | Key value Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 4), (3, 9)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform to Key value Pair\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize(range(4))\n",
    "final = data.map(lambda x: (x,x*x))\n",
    "final.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Actions on Key Value Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (2, 7)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summation of Key value Pair on the basis of Key\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([(1,2),(1,3),(2,4),(2,3)])\n",
    "final = data.reduceByKey(lambda x,y: (x+y))\n",
    "final.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, -1), (2, 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtraction of Key value Pair on the basis of Key\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = sc.parallelize([(1,2),(1,3),(2,4),(2,3)])\n",
    "final = data.reduceByKey(lambda x,y: (x-y))\n",
    "final.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', '1'), ('a', '1145')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = [(\"a\", 1), (\"b\", 1), (\"a\", 1),(\"a\", 45)]\n",
    "x = sc.parallelize(data)\n",
    "def add(a, b): return a + str(b)\n",
    "#sorted(x.combineByKey(str, add, add).collect())\n",
    "x.combineByKey(str, add, add).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (15.0, 3)), ('B', (30.0, 2)), ('Z', (28.0, 4))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "data = [\n",
    "        ('A', 2.), ('A', 4.), ('A', 9.), \n",
    "        ('B', 10.), ('B', 20.), \n",
    "        ('Z', 3.), ('Z', 5.), ('Z', 8.), ('Z', 12.) \n",
    "       ]\n",
    "\n",
    "rdd = sc.parallelize( data )\n",
    "sumCount = rdd.combineByKey(lambda value: (value, 1),\n",
    "                            lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                            lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    "                           )\n",
    "sumCount.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 4), ('b', 5)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract between 2 arrays\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None),(\"b\", 2)])\n",
    "#x.collect()\n",
    "#y.collect()\n",
    "sorted(x.subtract(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions on  RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add elements of array\n",
    "x= sc.parallelize([1.0, 2.0, 3.0])\n",
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81649658092772603"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find standard deviation of array values\n",
    "x= sc.parallelize([1.0, 2.0, 3.0])\n",
    "x.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find variation of array values\n",
    "x= sc.parallelize([1.0, 2.0, 3.0])\n",
    "x.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 4, mean: 2.5, stdev: 1.11803398875, max: 4.0, min: 1.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find stats array values\n",
    "x= sc.parallelize([1.0, 2.0, 3.0,4.0])\n",
    "x.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting 1 -s array to 2-d array by keeping y constant in (x,y)\n",
    "sc = SparkContext.getOrCreate()\n",
    "a = range(5) \n",
    "b = sc.parallelize (a)\n",
    "increment = b.map(lambda x: (x,1))\n",
    "increment.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (1, 1), (1, 2), (1, 3), (1, 4)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting 1 -s array to 2-d array by keeping X constant in (x,y)\n",
    "a = range(5) \n",
    "b = sc.parallelize (a)\n",
    "increment = b.map(lambda x: (1,x))\n",
    "increment.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (1, 3), (2, 4), (3, 5), (4, 6)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting 1 -d array to 2-d array by keeping X constant in (x,y) and increment each value of y by 2\n",
    "a = range(5) \n",
    "b = sc.parallelize (a)\n",
    "increment = b.map(lambda x: (x,x+2))\n",
    "increment.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Abhishek', 8), ('Alok', 4), ('Vishnu', 6)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract lenth of each element in array\n",
    "x = sc.parallelize([\"Abhishek\", \"Alok\", \"Vishnu\"])\n",
    "y = x.map(lambda x: (x,len(x)))\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abhishek 8\n",
      "Alok 4\n",
      "Vishnu 6\n",
      "Aliya 5\n",
      "Ram 3\n",
      "Ashwini 7\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Only python, in multiple nodes spark will be faster\n",
    "x = ([\"Abhishek\", \"Alok\", \"Vishnu\",  \"Aliya\", \"Ram\" , \"Ashwini\"])\n",
    "for i in x:\n",
    "    y = len(i)\n",
    "    print (i,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 6, mean: 5.5, stdev: 1.70782512766, max: 8.0, min: 3.0)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create stats of lenth of each element in array\n",
    "x = sc.parallelize([\"Abhishek\", \"Alok\", \"Vishnu\",  \"Aliya\", \"Ram\" , \"Ashwini\"])\n",
    "y=  x.map(lambda y:(len(y)))\n",
    "y.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 237, mean: 34.400843881856545, stdev: 68.5711592519, max: 635.0, min: 0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create stats of lenth of each element of data in text file\n",
    "data= sc.textFile(\"D:/E_drive/perseonal/Training/Spark/Data/strings.txt\")\n",
    "y=  data.map(lambda y:(len(y)))\n",
    "y.collect()\n",
    "y.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Operations on Text file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['YEAR', 'GoogleKnowlege_Occupation', 'Show', 'Group', 'Raw_Guest_List'],\n",
       " ['1999', 'actor', '01-11-1999', 'Acting', 'Michael J. Fox'],\n",
       " ['1999', 'Comedian', '01-12-1999', 'Comedy', 'Sandra Bernhard'],\n",
       " ['1999', 'television actress', '1/13/99', 'Acting', 'Tracey Ullman'],\n",
       " ['1999', 'film actress', '1/14/99', 'Acting', 'Gillian Anderson']]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract top 5 rows from text file (data is in tabular format)\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/OCP_data.tsv\")\n",
    "raw_data.take(1)\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "daily_show.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 1),\n",
       " ('2002', 159),\n",
       " ('2003', 166),\n",
       " ('2004', 164),\n",
       " ('2007', 141),\n",
       " ('2010', 165),\n",
       " ('2011', 163),\n",
       " ('2012', 164),\n",
       " ('2013', 166),\n",
       " ('2014', 163),\n",
       " ('2015', 100),\n",
       " ('1999', 166),\n",
       " ('2000', 169),\n",
       " ('2001', 157),\n",
       " ('2005', 162),\n",
       " ('2006', 161),\n",
       " ('2008', 164),\n",
       " ('2009', 163)]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No. of Shows per Year\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/OCP_data.tsv\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "tally = daily_show.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y: x+y)\n",
    "tally.take(tally.count())\n",
    "#tally.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR GoogleKnowlege_Occupation', 1),\n",
       " ('1999 actor', 53),\n",
       " ('1999 Comedian', 5),\n",
       " ('1999 television actress', 5),\n",
       " ('1999 Singer-lyricist', 1),\n",
       " ('1999 actress', 42),\n",
       " ('1999 Singer-songwriter', 2),\n",
       " ('1999 Comic', 1),\n",
       " ('1999 rock band', 2),\n",
       " ('1999 musician', 2)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many show per group of year and occupation\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/OCP_data.tsv\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "tally = daily_show.map(lambda x: (x[0]+' ' +x[1], 1))\n",
    "tally.reduceByKey(lambda x,y: x+y).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Exercise  on Automobil data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symboling normalized-losses make fuel-type aspiration num-of-doors body-style drive-wheels engine-location wheel-base length width height curb-weight engine-type num-of-cylinders engine-size fuel-system bore stroke compression-ratio horsepower peak-rpm city-mpg highway-mpg price',\n",
       " '3 ? alfa-romero gas std two convertible rwd front 88.6 168.8 64.1 48.8 2548 dohc four 130 mpfi 3.47 2.68 9 111 5000 21 27 13495']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/automobile.txt\")\n",
    "raw_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symboling',\n",
       " 'normalized-losses',\n",
       " 'make',\n",
       " 'fuel-type',\n",
       " 'aspiration',\n",
       " 'num-of-doors',\n",
       " 'body-style',\n",
       " 'drive-wheels',\n",
       " 'engine-location',\n",
       " 'wheel-base',\n",
       " 'length',\n",
       " 'width',\n",
       " 'height',\n",
       " 'curb-weight',\n",
       " 'engine-type',\n",
       " 'num-of-cylinders',\n",
       " 'engine-size',\n",
       " 'fuel-system',\n",
       " 'bore',\n",
       " 'stroke',\n",
       " 'compression-ratio',\n",
       " 'horsepower',\n",
       " 'peak-rpm',\n",
       " 'city-mpg',\n",
       " 'highway-mpg',\n",
       " 'price',\n",
       " '3',\n",
       " '?',\n",
       " 'alfa-romero',\n",
       " 'gas',\n",
       " 'std',\n",
       " 'two',\n",
       " 'convertible',\n",
       " 'rwd',\n",
       " 'front',\n",
       " '88.6',\n",
       " '168.8',\n",
       " '64.1',\n",
       " '48.8',\n",
       " '2548',\n",
       " 'dohc',\n",
       " 'four',\n",
       " '130',\n",
       " 'mpfi',\n",
       " '3.47',\n",
       " '2.68',\n",
       " '9',\n",
       " '111',\n",
       " '5000',\n",
       " '21',\n",
       " '27',\n",
       " '13495',\n",
       " '3',\n",
       " '?',\n",
       " 'alfa-romero',\n",
       " 'gas',\n",
       " 'std',\n",
       " 'two',\n",
       " 'convertible',\n",
       " 'rwd',\n",
       " 'front',\n",
       " '88.6',\n",
       " '168.8',\n",
       " '64.1',\n",
       " '48.8',\n",
       " '2548',\n",
       " 'dohc',\n",
       " 'four',\n",
       " '130',\n",
       " 'mpfi',\n",
       " '3.47',\n",
       " '2.68',\n",
       " '9',\n",
       " '111',\n",
       " '5000',\n",
       " '21',\n",
       " '27',\n",
       " '16500',\n",
       " '1',\n",
       " '?',\n",
       " 'alfa-romero',\n",
       " 'gas',\n",
       " 'std',\n",
       " 'two',\n",
       " 'hatchback',\n",
       " 'rwd',\n",
       " 'front',\n",
       " '94.5',\n",
       " '171.2',\n",
       " '65.5',\n",
       " '52.4',\n",
       " '2823',\n",
       " 'ohcv',\n",
       " 'six',\n",
       " '152',\n",
       " 'mpfi',\n",
       " '2.68',\n",
       " '3.47',\n",
       " '9',\n",
       " '154']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract top 5 rows from text file (data is in tabular format)\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/automobile.txt\")\n",
    "raw_data.take(1)\n",
    "daily_show = raw_data.flatMap(lambda line: line.split(' '))\n",
    "daily_show.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many show per group of year and occupation\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/automobile.txt\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "tally = daily_show.map(lambda x: (x[0]+' ' +x[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/sparkinstall/spark-2.2.0-bin-hadoop2.7/bin/Code/Data/automobile.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ae30ad0f7d8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtally\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\sparkinstall\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\sparkinstall\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\sparkinstall\\spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\sparkinstall\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/sparkinstall/spark-2.2.0-bin-hadoop2.7/bin/Code/Data/automobile.txt\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:194)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "tally.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many show per group of year and occupation\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/automobile.txt\")\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "tally = daily_show.map(lambda x: (x[0]+' ' +x[1], 1))\n",
    "tally.reduceByKey(lambda x,y: x+y).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91, 92, 93]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YEAR', 'GoogleKnowlege_Occupation'),\n",
       " ('1999', 'actor'),\n",
       " ('1999', 'Comedian'),\n",
       " ('1999', 'television actress'),\n",
       " ('1999', 'film actress')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In last update range works same as xrange\n",
    "sc = SparkContext.getOrCreate()\n",
    "raw_data = sc.textFile(\"./Data/OCP_data.tsv\")\n",
    "raw_data.take(1)\n",
    "daily_show = raw_data.map(lambda line: line.split('\\t'))\n",
    "K_data =daily_show.map(lambda k: (k[0],k[1]))\n",
    "#G_data =K_data.groupByKey().map(lambda k, j: (k,[x for x in j]))\n",
    "#G_data.take(5)\n",
    "K_data.take (5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide values for respective keys\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize ([(1,3),(2,3),(1,3),(2,3),(1,4),(2,8)])\n",
    "a.lookup(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 4, 2: 1, 3: 4}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide dictionary intead tupple\n",
    "sc = SparkContext.getOrCreate()\n",
    "a =sc.parallelize ([(1,3),(2,3),(1,3),(2,3),(1,4),(2,8),(3,4),(2,1)])\n",
    "a.collectAsMap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
